{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceafcc61",
   "metadata": {},
   "source": [
    "We are trying to **find a line (or direction)** that represents the data well.\n",
    "\n",
    "The function being minimized is:\n",
    "\n",
    "$$\n",
    "f(w) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - (x_i^T w) w \\right\\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "* $x_i$ is your data point.\n",
    "* $w$ is the direction (line) we are trying to find.\n",
    "* $x_i^T w$ gives projection of $x_i$ onto $w$.\n",
    "* $(x_i^T w) w$ is the point on the line closest to $x_i$.\n",
    "* So, the function measures how far each point is from the line.\n",
    "\n",
    "We want to **minimize this total distance**.\n",
    "\n",
    "$$\n",
    "\\left\\| x_i - (x_i^T w) w \\right\\|^2 = \\text{distance squared}\n",
    "$$\n",
    "\n",
    "Expanding it using norm properties gives:\n",
    "\n",
    "$$\n",
    "= (x_i^T x_i) - (x_i^T w)^2 - (x_i^T w)^2 + (x_i^T w)^2 \\cdot (w^T w)\n",
    "$$\n",
    "\n",
    "\n",
    "* $w^T w = 1$ (because $w$ is a unit vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefddae",
   "metadata": {},
   "source": [
    "### What is a **norm**?\n",
    "\n",
    "A **norm** is a way to measure the **length** or **size** of a vector.\n",
    "\n",
    "For a vector $v = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$, the **Euclidean norm** is:\n",
    "\n",
    "$$\n",
    "\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n",
    "$$\n",
    "\n",
    "It’s the straight-line distance from the origin to the point $v$ in space.\n",
    "\n",
    "For any vector $v$, this is true:\n",
    "\n",
    "$$\n",
    "\\|v\\|^2 = v^T v\n",
    "$$\n",
    "\n",
    "Why?\n",
    "Because:\n",
    "\n",
    "$$\n",
    "\\|v\\|^2 = (v_1^2 + v_2^2 + \\dots + v_n^2) = v^T v\n",
    "$$\n",
    "\n",
    "So we use this to expand things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d6030",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Expand\n",
    "\n",
    "$$\n",
    "\\left\\| x_i - (x_i^T w) w \\right\\|^2\n",
    "$$\n",
    "\n",
    "Let’s call:\n",
    "\n",
    "* $a = x_i$\n",
    "* $b = (x_i^T w) w$\n",
    "\n",
    "So we are expanding $\\|a - b\\|^2$\n",
    "\n",
    "\n",
    "$$\n",
    "\\|a - b\\|^2 = (a - b)^T (a - b)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "(a - b)^T (a - b) = a^T a - 2a^T b + b^T b\n",
    "$$\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "* $a^T a = x_i^T x_i$\n",
    "* $a^T b = x_i^T (x_i^T w) w = (x_i^T w)^2$\n",
    "* $b^T b = ((x_i^T w) w)^T ((x_i^T w) w) = (x_i^T w)^2 (w^T w)$\n",
    "\n",
    "And $w^T w = 1$ because $w$ is a unit vector.\n",
    "\n",
    "### Final result:\n",
    "\n",
    "$$\n",
    "\\left\\| x_i - (x_i^T w) w \\right\\|^2 = x_i^T x_i - 2(x_i^T w)^2 + (x_i^T w)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= x_i^T x_i - (x_i^T w)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094e472",
   "metadata": {},
   "source": [
    "$$\n",
    "b^T b = \\left( (x_i^T w) w \\right)^T \\left( (x_i^T w) w \\right)\n",
    "$$\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "* $s = x_i^T w$ (this is just a scalar)\n",
    "* $b = s \\cdot w$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "b^T b = (s w)^T (s w)\n",
    "$$\n",
    "\n",
    "We can factor out the scalar:\n",
    "\n",
    "$$\n",
    "= s^2 \\cdot (w^T w)\n",
    "$$\n",
    "\n",
    "Now, since $w$ is a **unit vector**, $w^T w = 1$.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "b^T b = s^2 \\cdot 1 = (x_i^T w)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc14347",
   "metadata": {},
   "source": [
    "### Drop the constant\n",
    "\n",
    "We had:\n",
    "\n",
    "$$\n",
    "f(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ x_i^T x_i - (x_i^T w)^2 \\right]\n",
    "$$\n",
    "\n",
    "But $x_i^T x_i$ does **not depend on** $w$. So it’s a **constant** during optimization.\n",
    "\n",
    "So minimizing $f(w)$ is the same as **minimizing only**:\n",
    "\n",
    "$$\n",
    "g(w) = \\frac{1}{n} \\sum_{i=1}^{n} - (x_i^T w)^2\n",
    "$$\n",
    "\n",
    "And now we **maximize** $(x_i^T w)^2$, since it had a negative sign.\n",
    "\n",
    "So:\n",
    "**Minimizing projection error ⇔ Maximizing projected variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5f4a8",
   "metadata": {},
   "source": [
    "#### 1. What is projection error?\n",
    "\n",
    "From before:\n",
    "\n",
    "$$\n",
    "\\left\\| x_i - (x_i^T w) w \\right\\|^2\n",
    "$$\n",
    "\n",
    "This is the **squared distance** between the point $x_i$ and its projection on direction $w$.\n",
    "This is called **projection error**.\n",
    "\n",
    "\n",
    "#### 2. What is projected variance?\n",
    "\n",
    "$$\n",
    "(x_i^T w)^2\n",
    "$$\n",
    "\n",
    "This is the **square of the projection** of $x_i$ on $w$.\n",
    "If you average these over all data points, you get the **variance** in direction $w$.\n",
    "\n",
    "\n",
    "#### 3. The connection\n",
    "\n",
    "You had:\n",
    "\n",
    "$$\n",
    "f(w) = \\frac{1}{n} \\sum \\left[ x_i^T x_i - (x_i^T w)^2 \\right]\n",
    "$$\n",
    "\n",
    "The first term $x_i^T x_i$ is constant (doesn’t depend on $w$).\n",
    "So minimizing $f(w)$ means maximizing the **second term**:\n",
    "\n",
    "$$\n",
    "(x_i^T w)^2\n",
    "$$\n",
    "\n",
    "That’s the **projected variance**.\n",
    "\n",
    "\n",
    "So:\n",
    "**Minimize error** ⇔ **Maximize projection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f615775",
   "metadata": {},
   "source": [
    "#### What Are Eigenvectors and Eigenvalues?\n",
    "\n",
    "Eigenvectors and eigenvalues are concepts from linear algebra that help us understand how matrices transform vectors.\n",
    "\n",
    "#### What’s a Matrix?\n",
    "\n",
    "A matrix is like a table of numbers that can transform vectors. For example, if you have a 2D vector $v = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$, a 2×2 matrix $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ can transform $v$ into a new vector by matrix multiplication: \n",
    "\n",
    "$$\n",
    "A v = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} ax + by \\\\ cx + dy \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This transformation might stretch, rotate, or flip the vector $v$. Eigenvectors and eigenvalues describe special cases of this transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8860f2",
   "metadata": {},
   "source": [
    "#### Eigenvector: A Special Direction\n",
    "An **eigenvector** of a matrix $A$ is a vector that, when transformed by $A$, doesn’t change its direction—it only gets stretched or shrunk (or flipped). Mathematically:\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "- $v$ is the eigenvector.\n",
    "- $\\lambda$ (a scalar) is the eigenvalue.\n",
    "- This equation says: “When I apply the matrix $A$ to the vector $v$, I get the same vector $v$, just scaled by $\\lambda$.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7c6a4",
   "metadata": {},
   "source": [
    "#### Eigenvalue: The Scaling Factor\n",
    "The **eigenvalue** $ \\lambda $ tells you how much the eigenvector is stretched (or shrunk):\n",
    "- If $ \\lambda = 2 $, the eigenvector is stretched to twice its length.\n",
    "- If $ \\lambda = 0.5 $, the eigenvector is shrunk to half its length.\n",
    "- If $ \\lambda = -1 $, the eigenvector is flipped (same direction, opposite orientation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950819d",
   "metadata": {},
   "source": [
    "#### Simple Example\n",
    "Let’s take a 2×2 matrix:\n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- If we apply $A$ to the vector $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$:\n",
    "  $$\n",
    "  A v_1 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n",
    "  \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
    "  = \\begin{bmatrix} 2 \\cdot 1 + 0 \\cdot 0 \\\\ 0 \\cdot 1 + 3 \\cdot 0 \\end{bmatrix}\n",
    "  = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\n",
    "  = 2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
    "  $$\n",
    "  The vector $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ is an eigenvector because it didn’t change direction (it’s still along the x-axis), and it was scaled by $\\lambda = 2$. So, the eigenvalue is 2.\n",
    "\n",
    "- Now try $v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$:\n",
    "  $$\n",
    "  A v_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n",
    "  \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "  = \\begin{bmatrix} 2 \\cdot 0 + 0 \\cdot 1 \\\\ 0 \\cdot 0 + 3 \\cdot 1 \\end{bmatrix}\n",
    "  = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}\n",
    "  = 3 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "  $$\n",
    "  The vector $v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ is also an eigenvector, with eigenvalue $\\lambda = 3$.\n",
    "\n",
    "This matrix has two eigenvectors, each with its own eigenvalue. In general, a $d \\times d$ matrix can have up to $d$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc94e8a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_{||w||_2=1} w^{\\top} C w\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "C = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^{\\top}\n",
    "$$\n",
    "\n",
    "And the solution is:\n",
    "- $w$ is the eigenvector corresponding to the maximum eigenvalue of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b491f",
   "metadata": {},
   "source": [
    "#### **What is $C$?**\n",
    "\n",
    "- $x_i$ is a $d \\times 1$ vector (a data point with $d$ dimensions).\n",
    "- $x_i x_i^{\\top}$ is a $d \\times d$ matrix (the outer product of $x_i$ with itself).\n",
    "- $C = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^{\\top}$ is the average of $n$ such matrices, so $C$ is also a $d \\times d$ matrix.\n",
    "- The slide labels $C$ as a \"covariance matrix.\" In machine learning, this matrix captures how the dimensions of the data points $x_i$ vary together.  \n",
    "  (If the data is centered—i.e., the mean of $x_i$ is zero—then $C$ is exactly the covariance matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538bb1e",
   "metadata": {},
   "source": [
    "#### **The Optimization Problem**\n",
    "\n",
    "The expression \n",
    "$$\n",
    "\\max_{\\|w\\|^2=1} w^{\\top} C w\n",
    "$$ \n",
    "can be interpreted as follows:\n",
    "\n",
    "- $w$ is a vector in $\\mathbb{R}^d$ with unit norm, i.e., $\\|w\\|^2=1$.\n",
    "- The term $w^{\\top} C w$ is computed by multiplying:\n",
    "  - $w^{\\top}$, a $1 \\times d$ row vector,\n",
    "  - $C$, a $d \\times d$ matrix,\n",
    "  - $w$, a $d \\times 1$ column vector,\n",
    "  \n",
    "  which results in a scalar.\n",
    "\n",
    "The goal is to find the unit vector $w$ that maximizes this scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257c6ba",
   "metadata": {},
   "source": [
    "- The maximum value of $w^\\top C w$ (subject to $\\|w\\|^2 = 1$) is achieved when $w$ is the eigenvector of $C$ corresponding to the largest eigenvalue.  \n",
    "- In that case, the value of $w^\\top C w$ is exactly the largest eigenvalue.\n",
    "\n",
    "**Why does this work?**\n",
    "- If $w$ is an eigenvector of $C$ with eigenvalue $\\lambda$, then  \n",
    "  $$\n",
    "  Cw = \\lambda w\n",
    "  $$\n",
    "\n",
    "- Now compute $w^\\top C w$:  \n",
    "  $$\n",
    "  w^\\top C w = w^\\top (\\lambda w) = \\lambda \\, (w^\\top w)\n",
    "  $$\n",
    "\n",
    "- Since $\\|w\\|^2 = 1$, we have  \n",
    "  $$\n",
    "  w^\\top w = 1,\n",
    "  $$  \n",
    "  so  \n",
    "  $$\n",
    "  w^\\top C w = \\lambda.\n",
    "  $$\n",
    "\n",
    "- Therefore, $w^\\top C w$ equals the eigenvalue $\\lambda$.  \n",
    "- To maximize $w^\\top C w$, we choose $w$ to be the eigenvector corresponding to the maximum eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e48279",
   "metadata": {},
   "source": [
    "**Eigenvectors of a symmetric matrix form a basis**\n",
    "\n",
    "Let’s say your data has $d = 3$ features. So $\\mathbf{w} \\in \\mathbb{R}^3$\n",
    "\n",
    "The covariance matrix $\\mathbf{C}$ is symmetric → It has 3 real, orthonormal eigenvectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\n",
    "$$\n",
    "\n",
    "Any vector $\\mathbf{w} \\in \\mathbb{R}^3$ can be written as a combination of them:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = a_1 \\mathbf{v}_1 + a_2 \\mathbf{v}_2 + a_3 \\mathbf{v}_3\n",
    "$$\n",
    "\n",
    "This is just like saying any 3D vector can be written using standard basis:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = w_1 \\hat{i} + w_2 \\hat{j} + w_3 \\hat{k}\n",
    "$$\n",
    "\n",
    "Only now the basis is $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
