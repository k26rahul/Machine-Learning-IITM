{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcded7b1",
   "metadata": {},
   "source": [
    "MLT week 1\n",
    "\n",
    "L1 Intro to machine learning\n",
    "\n",
    "Traditional programming is procedural, and it's about memorizing steps for logic.\n",
    "Machine learning is data driven, and you have to generalise the model.\n",
    "\n",
    "L2 Paradigms of machine learning\n",
    "\n",
    "Supervised learning (labelled data is used)\n",
    "\n",
    "- Regression, predicting numeric values, Boston housing project\n",
    "- Classification, predicting categories (classes), MAGIC telescope project, binary vs multi\n",
    "\n",
    "Unsupervised learning\n",
    "\n",
    "- Clustering, finding clusters in data\n",
    "- Representation learning, compression of data\n",
    "\n",
    "Sequential learning (not in syllabus)\n",
    "\n",
    "- Online learning, always keep learning\n",
    "- Reinforcement learning, feedback based learning\n",
    "\n",
    "Foundations of machine learning\n",
    "\n",
    "- Structure of data, linear algebra\n",
    "- Uncertainty, probability\n",
    "- Decision, optimisation (calculus)\n",
    "\n",
    "L3 Representation learning part 1\n",
    "\n",
    "It's unsupervised ( you aren't predicting anything, so labels aren't required). It's about understanding the data.\n",
    "\n",
    "The goal is to compress the data. Reduce the number of features, and still maintain the structure/order/nature/variance of data.\n",
    "\n",
    "Data samples are points in a coordinate system. These points can be vectors too.\n",
    "\n",
    "Vectors represent the lines in their direction. Vectors can be scaled (by multiplying a scalar to that vector) to get any point on that line.\n",
    "\n",
    "Example of compression: Suppose you've 10 samples with 3 features. You'll need to keep 30 numbers to store this data. You can choose a \"representative\" vector w in 3D space. Then calculate the scalar proxies (c values) of 10 samples on your chosen w. Finally you'll have 3 numbers for your w, and 10 coefficients for each sample, so total 13 numbers, but earlier you had 30 numbers. So data is compressed.\n",
    "\n",
    "Finding proxy of a sample (x) on your chosen line (w)\n",
    "\n",
    "- Draw perpendicular from x on w. To find the point (proxy) on w closest to x\n",
    "- Proxy can be obtained by scaling w by scalar c. Derive equation for c.\n",
    "\n",
    "L4 Representation learning part 2\n",
    "\n",
    "Finding the best representative (w)\n",
    "\n",
    "- Assume w to be unit a unit vector. Now define error as distance between proxies and data points.\n",
    "- Minimize the average error, or maximize the variance in proxies.\n",
    "- Best w = argmax w' _ C _ w.\n",
    "- C is the covariance matrix XX'. X is d \\* n data matrix.\n",
    "- Best w is the eigen vector corresponding to the largest eigen value of C.\n",
    "\n",
    "L5 Representation learning part 3\n",
    "\n",
    "- After finding the best w, observe that residuals also have some information (variance). And residuals are perpendicular to w.\n",
    "- So we'll take another vector w2 perpendicular to w1. Find proxies for samples on w2.\n",
    "- Then find w3 perpendicular to both w1 and w2. Find proxies of samples on w3.\n",
    "- Keep repeating this for w4, w5... Etc. But how many times to repeat? Till variance of proxies in the chosen direction (some w) becomes zero. Atmost d (# of features) times.\n",
    "- We call them principal components. They are orthogonal. They are in decreasing order of \"variance explained\". PC1 explains maximum variance, then PC2 and so on.\n",
    "- These are in fact eigen vectors of covariance matrix C in decreasing order of eigen values.\n",
    "- Data must be centered before doing anything. Mean subtract all features. So the new mean is zero for all features.\n",
    "\n",
    "L6 Principal component analysis part 1\n",
    "\n",
    "Residues will slowly become zero. It's guaranteed to become zero after d steps. But it may become zero sooner than that, meaning our data lives in a low dimensional linear subspace. Example: data lives in a plane (2D) in 3D space. For 1000 samples you'll store 3000 numbers. But after PCA you transformed data to have two components only (in best directions to explain maximum/all variance), then you'll store only 2000 + 3 + 3 numbers. In this example you captured all variance in just 2 components. So your reconstruction loss is zero.\n",
    "\n",
    "But what if you cannot capture all variance in less than d components? Then you'll make some compromises. You'll choose the best k components and suffer some reconstruction loss.\n",
    "\n",
    "Best k? Pick top k components that collectively explain 95% of variance.\n",
    "\n",
    "L7 Principal component analysis part 2\n",
    "\n",
    "Error minimisation on centered dataset <=> Variance maximization\n",
    "\n",
    "PCA finds combination of features that are decorrelated. PCs are independent/orthogonal of each other. These are the \"basis\" of reduced vector space.\n",
    "\n",
    "PCA is \"change of basis\".\n",
    "\n",
    "PCA is algorithm used for \"dimensionality reduction\".\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
